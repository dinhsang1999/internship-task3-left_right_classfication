{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "9d9f1b1c3e98cd584c7abc6dfa1a222a03734772"
   },
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = height * 1.5\n",
    "dataset_path = \"./preprocess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7f33a3700cc60d61c6b9581a9721589d68fc16a4"
   },
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if 'left' in filename or 'right' in filename:\n",
    "        file_paths.append(os.path.join(dataset_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "89cbb8eb8b01ba873de9bd8d63be7a39d5c2031e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "3dad3542b836d50fb01c97aa94fad23ad150b241"
   },
   "outputs": [],
   "source": [
    "shuffle(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "55ba1e4da74549117da3e7afd626af1c25e905d5"
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_files = file_paths[:int(len(file_paths)*train_ratio)]\n",
    "val_files = file_paths[int(len(file_paths)*train_ratio):int(len(file_paths)*(train_ratio + val_ratio))]\n",
    "test_files = file_paths[int(len(file_paths)*(train_ratio + val_ratio)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "c897a82ea28331b893597e84f9814d4d5d0a57d5"
   },
   "outputs": [],
   "source": [
    "class FundusDataset(utils.Dataset):   \n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths_list = image_paths \n",
    "        # List of image paths      \n",
    "        self.labels_list = [] \n",
    "        # List of labels correlated      \n",
    "        self.transform = transform \n",
    "        # Transformation applying to each data piece            \n",
    "        # Run through the folder and get the label of each image inside  \n",
    "        for filename in image_paths:\n",
    "            self.labels_list.append([1,0] if 'left' in filename else [0,1])\n",
    "        \n",
    "    def __getitem__(self, index):      \n",
    "        '''      Is called when get DataLoader iterated      '''      \n",
    "        # Get image path with index      \n",
    "        image_path = self.image_paths_list[index]      \n",
    "        # Read image with Pillow library      \n",
    "        image = Image.open(image_path).convert('RGB')      \n",
    "        # Get label      \n",
    "        image_label = torch.FloatTensor(self.labels_list[index])\n",
    "        # Post-transformation apply for image      \n",
    "        if self.transform != None:          \n",
    "            image = self.transform(image)            \n",
    "        return image, image_label      \n",
    "    def __len__(self):      \n",
    "        return len(self.image_paths_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "85eec0d52795c344bf89a1f2840c379d5bbe30e6"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((int(width), int(height))),                                \n",
    "                                transforms.ToTensor(),                                \n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "b789a661a566b699f09f7ea9bcc8dcfbd0a77c6f"
   },
   "outputs": [],
   "source": [
    "train_dataset = FundusDataset(train_files, transform)\n",
    "trainloader = utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "08ca70ef1608ba993925b43ee54b885e533009cb"
   },
   "outputs": [],
   "source": [
    "val_dataset = FundusDataset(val_files, transform)\n",
    "valloader = utils.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "1c6511c7a953086be797bf74cd936822c665c9bc"
   },
   "outputs": [],
   "source": [
    "test_dataset = FundusDataset(test_files, transform)\n",
    "testloader = utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "e9588aed56161598a92831d955fc9d8648cbbe9c"
   },
   "outputs": [],
   "source": [
    "class FundusNet(nn.Module):\n",
    "    def __init__(self, is_trained):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=is_trained)\n",
    "        kernel_count = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(nn.Linear(2560, 2),nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f2675bfe0a3a3ab6d6c9583a938816b71b22b149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:    \n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:    \n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d6644e09dc26ae297f8e476ed3ca0c4445e60c4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/Projects/env/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "model = FundusNet(True)\n",
    "if train_on_gpu:\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "#state_dict = torch.load('best_model.pth')\n",
    "#model.load_state_dict(state_dict)\n",
    "loss = nn.BCELoss(size_average = True)\n",
    "optimizer = optim.Adam (model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "6848d5a07cca0448c5ae3200f882637030f83c94",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100... Validating step 70/70... Loss 0.050665594637393956\n",
      "Epoch: 1/100..  Training Loss: 0.049..  Val Loss: 0.051..  Val Accuracy: 0.986\n",
      "Improve loss of model from 999999999 to 0.05066559463739395\n",
      "Epoch 2/100... Validating step 70/70... Loss 0.048595219850540165\n",
      "Epoch: 2/100..  Training Loss: 0.045..  Val Loss: 0.049..  Val Accuracy: 0.985\n",
      "Improve loss of model from 0.05066559463739395 to 0.04859521985054016\n",
      "Epoch 3/100... Validating step 70/70... Loss 0.045802999287843704\n",
      "Epoch: 3/100..  Training Loss: 0.042..  Val Loss: 0.046..  Val Accuracy: 0.988\n",
      "Improve loss of model from 0.04859521985054016 to 0.045802999287843704\n",
      "Epoch 4/100... Validating step 70/70... Loss 0.047558862715959554\n",
      "Epoch: 4/100..  Training Loss: 0.042..  Val Loss: 0.048..  Val Accuracy: 0.986\n",
      "Epoch 5/100... Validating step 70/70... Loss 0.049132466316223145\n",
      "Epoch: 5/100..  Training Loss: 0.040..  Val Loss: 0.049..  Val Accuracy: 0.986\n",
      "Epoch 6/100... Validating step 70/70... Loss 0.048398818820714954\n",
      "Epoch: 6/100..  Training Loss: 0.038..  Val Loss: 0.048..  Val Accuracy: 0.987\n",
      "Epoch 7/100... Validating step 70/70... Loss 0.045804809778928764\n",
      "Epoch: 7/100..  Training Loss: 0.036..  Val Loss: 0.046..  Val Accuracy: 0.986\n",
      "Epoch 8/100... Validating step 70/70... Loss 0.042887646704912186\n",
      "Epoch: 8/100..  Training Loss: 0.035..  Val Loss: 0.043..  Val Accuracy: 0.989\n",
      "Improve loss of model from 0.045802999287843704 to 0.042887646704912186\n",
      "Epoch 9/100... Validating step 70/70... Loss 0.051504351198673254\n",
      "Epoch: 9/100..  Training Loss: 0.032..  Val Loss: 0.052..  Val Accuracy: 0.987\n",
      "Epoch 10/100... Validating step 70/70... Loss 0.049332454800605774\n",
      "Epoch: 10/100..  Training Loss: 0.029..  Val Loss: 0.049..  Val Accuracy: 0.988\n",
      "Epoch 11/100... Validating step 70/70... Loss 0.052181057631969455\n",
      "Epoch: 11/100..  Training Loss: 0.025..  Val Loss: 0.052..  Val Accuracy: 0.988\n",
      "Epoch 12/100... Validating step 70/70... Loss 0.060737766325473785\n",
      "Epoch: 12/100..  Training Loss: 0.021..  Val Loss: 0.061..  Val Accuracy: 0.987\n",
      "Epoch 13/100... Validating step 70/70... Loss 0.052633427083492285\n",
      "Epoch: 13/100..  Training Loss: 0.019..  Val Loss: 0.053..  Val Accuracy: 0.990\n",
      "Epoch 14/100... Validating step 70/70... Loss 0.06020200625061989Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "Epoch: 14/100..  Training Loss: 0.014..  Val Loss: 0.060..  Val Accuracy: 0.989\n",
      "Epoch 15/100... Validating step 70/70... Loss 0.0652188137173652665\n",
      "Epoch: 15/100..  Training Loss: 0.006..  Val Loss: 0.065..  Val Accuracy: 0.990\n",
      "Epoch 16/100... Validating step 70/70... Loss 0.0738905668258667747\n",
      "Epoch: 16/100..  Training Loss: 0.002..  Val Loss: 0.074..  Val Accuracy: 0.990\n",
      "Epoch 17/100... Validating step 70/70... Loss 0.0781899839639663775\n",
      "Epoch: 17/100..  Training Loss: 0.001..  Val Loss: 0.078..  Val Accuracy: 0.991\n",
      "Epoch 18/100... Validating step 70/70... Loss 0.0899760648608207722\n",
      "Epoch: 18/100..  Training Loss: 0.001..  Val Loss: 0.090..  Val Accuracy: 0.990\n",
      "Epoch 19/100... Validating step 70/70... Loss 0.09423210471868515447\n",
      "Epoch: 19/100..  Training Loss: 0.000..  Val Loss: 0.094..  Val Accuracy: 0.990\n",
      "Epoch 20/100... Validating step 70/70... Loss 0.10073752701282501Epoch    19: reducing learning rate of group 0 to 1.0000e-05.\n",
      "\n",
      "Epoch: 20/100..  Training Loss: 0.000..  Val Loss: 0.101..  Val Accuracy: 0.990\n",
      "Epoch 21/100... Validating step 70/70... Loss 0.10056113451719284813\n",
      "Epoch: 21/100..  Training Loss: 0.000..  Val Loss: 0.101..  Val Accuracy: 0.989\n",
      "Epoch 22/100... Validating step 70/70... Loss 0.10308431833982468108\n",
      "Epoch: 22/100..  Training Loss: 0.000..  Val Loss: 0.103..  Val Accuracy: 0.990\n",
      "Epoch 23/100... Validating step 70/70... Loss 0.10361278802156448218\n",
      "Epoch: 23/100..  Training Loss: 0.000..  Val Loss: 0.104..  Val Accuracy: 0.990\n",
      "Epoch 24/100... Validating step 70/70... Loss 0.10668036341667175666\n",
      "Epoch: 24/100..  Training Loss: 0.000..  Val Loss: 0.107..  Val Accuracy: 0.990\n",
      "Epoch 25/100... Training step 544/555... Loss 0.00010358353236326976"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f985f523aded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_on_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-48e1f9e13130>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Post-transformation apply for image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/env/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m     def rotate(self, angle, resample=NEAREST, expand=0, center=None,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "best_loss = 999999999\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for step, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        if train_on_gpu:               \n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        ps = model(images)            \n",
    "        loss_val = loss(ps, labels)\n",
    "        loss_val.backward()            \n",
    "        optimizer.step()\n",
    "        running_loss += loss_val.item()\n",
    "        sys.stdout.write(f\"\\rEpoch {e+1}/{epochs}... Training step {step+1}/{len(trainloader)}... Loss {running_loss/(step+1)}\")\n",
    "    else:\n",
    "        val_loss = 0            \n",
    "        accuracy = 0\n",
    "        with torch.no_grad():                \n",
    "            for step, (images, labels) in enumerate(valloader):                    \n",
    "                if train_on_gpu:                       \n",
    "                    images, labels = images.cuda(), labels.cuda()                    \n",
    "                log_ps = model(images)\n",
    "                val_loss += loss(log_ps, labels)\n",
    "                ps = torch.exp(log_ps)                    \n",
    "                top_p, top_class = ps.topk(1, dim=1)                    \n",
    "                equals = top_class == torch.argmax(labels, dim=1).view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                sys.stdout.write(f\"\\rEpoch {e+1}/{epochs}... Validating step {step+1}/{len(valloader)}... Loss {val_loss/(step+1)}\")\n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        val_losses.append(val_loss/len(valloader))\n",
    "        scheduler.step(val_loss/len(valloader))\n",
    "        print(\"\\nEpoch: {}/{}.. \".format(e+1, epochs),                  \n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),                  \n",
    "              \"Val Loss: {:.3f}.. \".format(val_loss/len(valloader)),                  \n",
    "              \"Val Accuracy: {:.3f}\".format(accuracy/len(valloader)))\n",
    "        if best_loss > val_loss/len(valloader):\n",
    "            print(\"Improve loss of model from {} to {}\".format(best_loss, val_loss/len(valloader)))\n",
    "            best_loss = val_loss/len(valloader)\n",
    "            torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7d1573deec83d0181010bc402b9651ff2b69269"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bca3fdf2104f5749d3b786af2ecfca2a0eb3b3db"
   },
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
